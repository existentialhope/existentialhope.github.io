<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">

    <!--Description-->
    
        <meta name="description" content="&amp;quot;A great design appears at first insane; 
But chance will soon seem quaint and blind, 
And such an exemplary thinking brain 
Will soon by thinker">
    

    <!--Author-->
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="AI &amp; Cyberspace">
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Existential Hope">

    <!--Page Cover-->
    
        <meta property="og:image" content="">
    

    <!-- Title -->
    
    <title>AI &amp; Cyberspace - Existential Hope</title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/reset.css">
    <link rel="stylesheet" href="/css/main.css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.css" type="text/css" rel="stylesheet">

    <!-- Google Analytics -->
    


    <!--Favicon-->
    

</head>

<body>

<!-- Menu -->
<!-- Navigation -->
<header>
    <div class="logo">
        <a href="/">Existential Hope</a>
    </div><!-- end logo -->

    <div id="menu_icon"></div>
    <nav>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives">Archives</a>
            </li>
            
        </ul>
    </nav><!-- end navigation menu -->

    <div class="footer clearfix">
        <ul class="social clearfix">
            
            
                <li><a href="https://www.facebook.com/" class="fb" target="_blank" data-title="Facebook"></a></li>
            
            
                <li><a href="https://www.behance.net/" class="behance" target="_blank" data-title="Behance"></a></li>
            
            
                <li><a href="https://plus.google.com/+Pixelhint/posts" class="google" target="_blank" data-title="Google+"></a></li>
            
            
                <li><a href="https://dribbble.com/pixelhint" class="dribble" target="_blank" data-title="Dribble"></a></li>
            
            
            
            
        </ul><!-- end social -->

        <div class="rights">
            <p>Copyright Â© 2014 magnetic.</p>
            <p>Template by <a href="http://pixelhint.com/magnetic-free-html5-responsive-photography-website-template/">Pixelhint.com</a></p>
            <p>Hexo Theme by <a href="http://www.codeblocq.com/">Jonathan K.</a></p>
        </div><!-- end rights -->
    </div><!-- end footer -->
</header><!-- end header -->


<!-- Main Content -->
<section class="main clearfix">

    <section class="top" style="background: url('http://placehold.it/1300x500');">
        <div class="wrapper content_header clearfix">
            

<div class="work_nav">

    <ul class="btn clearfix">
        
        <li><a class="previous disabled"></a></li>
        
        <li><a href="/" class="grid" data-title="Portfolio"></a></li>
        
        <li><a class="next disabled"></a></li>
        
    </ul>

</div><!-- end work_nav -->
            <h1 class="title">AI & Cyberspace</h1>
        </div>
    </section><!-- end top -->

    <section class="wrapper">
        <div class="content">

            <!-- Gallery -->
            

            <!-- Content -->
            <p>&quot;A great design appears at first insane; <br>
But chance will soon seem quaint and blind, <br>
And such an exemplary thinking brain <br>
Will soon by thinkers be designed&quot; <br>
<em>- Goethe, 'Faust'</em></p>
<h2>Overview</h2>
<hr>
<h2>Start here</h2>
<ul>
<li>**Read: **<a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html" target="_blank" rel="noopener">The Artificial Intelligence Revolution </a>-  Wait But Why. Introductory post summarizing the potential dangers of Artificial General Intelligence</li>
<li>**Listen: **<a href="https://www.youtube.com/watch?v=AaNLX71Hl88&amp;t=6011s" target="_blank" rel="noopener">Sam Harris &amp; Eliezer Yudkowsky </a>- Waking Up. Good intro to problems in AI safety</li>
<li><strong>Do:</strong> <a href="http://aisafety.com/" target="_blank" rel="noopener">AIsafety.com</a> - virtual reading list on AI safety, meets every Wednesday.</li>
<li><strong>New:</strong> <a href="https://img1.wsimg.com/blobby/go/3d82daa4-97fe-4096-9c6b-376b92c619de/downloads/1c6q2kc4v_50335.pdf" target="_blank" rel="noopener">Malicious Use of AI Report</a> Miles Brundage et al.</li>
</ul>
<hr>
<h2>Summary</h2>
<p><strong>Intro</strong></p>
<ul>
<li><a href="http://www.calculemus.org/lect/08szt-intel/materialy/Definitions%20of%20Intelligence.html" target="_blank" rel="noopener">71 Definitions of Intelligence</a> - Shane Legg. Also: <a href="https://intelligence.org/2013/06/19/what-is-intelligence-2/" target="_blank" rel="noopener">What is intelligence?</a> Luke Muehlhauser on intelligence as optimization power divided by resources used.</li>
<li><a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html" target="_blank" rel="noopener">The Artificial Intelligence Revolution </a>-  Wait But Why. Introductory post summarizing the potential dangers of Artificial General Intelligence</li>
<li><a href="https://www.amazon.de/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111" target="_blank" rel="noopener">Superintelligence</a> - e-book by Nick Bostrom. Classic on Superintelligence, that explores different take-off scenarios of AI, why they're dangerous and urges for preparatory action now.</li>
<li><a href="http://lesswrong.com/lw/qk/that_alien_message/" target="_blank" rel="noopener">That Alien Message</a> - Eliezer Yudkowsky. Challenges mainstream human intuitions on speed of AI.</li>
<li><a href="https://www.youtube.com/watch?v=AaNLX71Hl88&amp;t=6011s" target="_blank" rel="noopener">Sam Harris &amp; Eliezer Yudkowsky </a>- good intro on common problems in AI safety</li>
</ul>
<p><strong>Progress &amp; Speed</strong></p>
<ul>
<li><a href="https://sideways-view.com/2018/02/24/takeoff-speeds/" target="_blank" rel="noopener">Slow Takeoff vs Fast Takeoff</a> - Paul Christiano</li>
<li><a href="https://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate" target="_blank" rel="noopener">The AI Foom Debate between Yudkowsky &amp; Hanson</a>. Hard or slow take off? Also as video here: <a href="https://www.youtube.com/watch?v=TuXl-iidnFY" target="_blank" rel="noopener">Yudkowsky vs Hanson: Singularity Debate</a>. As background: <a href="https://intelligence.org/files/IEM.pdf" target="_blank" rel="noopener">Intelligence Explosion Microeconomics </a>- Eliezer Yudkowsky. Suggests a methods to test whether a sufficiently advanced machine intelligence could build a smarter version of itself,</li>
<li>which could in turn build an even smarter version, and that this process could continue to the point of vastly surpassing human capability.</li>
</ul>
<h2></h2>
<hr>
<p>Risks</p>
<ul>
<li><a href="https://img1.wsimg.com/blobby/go/3d82daa4-97fe-4096-9c6b-376b92c619de/downloads/1c6q2kc4v_50335.pdf" target="_blank" rel="noopener">Malicious Use of AI Report</a> Miles Brundage and many other authors</li>
<li><a href="http://conferences.asucollegeoflaw.com/sciencepublicsphere/files/2014/02/intelligence.org_files_AIPosNegFactor.pdf" target="_blank" rel="noopener">AI as positive and negative factor in Global Risk</a> - Eliezer Yudkowsky</li>
<li><a href="https://intelligence.org/2017/10/13/fire-alarm/" target="_blank" rel="noopener">There is no fire alarm for AI safety</a> - Eliezer Yudkowsky</li>
<li>Many chapters in <a href="https://intelligence.org/rationality-ai-zombies/" target="_blank" rel="noopener">Rationality: From AI to Zombies</a> discuss AI Risk. Especially good are My Naturalist Awakening, That Tiny Note of Discord, Sympathetic Minds, Sorting Pebbles into Heaps, No Universally Compelling Argument, The Design of Minds in General, Detached Lever Fallacy, Ethical Injunctions, Magical Theories, Fake Utility Functions.</li>
<li><a href="https://wiki.lesswrong.com/wiki/Orthogonality_thesis" target="_blank" rel="noopener">Orthogonality Thesis </a>- Bostrom, Sandberg, Douglas. Why Intelligence and goals are orthogonal. Also: <a href="https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf" target="_blank" rel="noopener">Basic AI drives</a> - Steve Omohundro. sufficiently advanced AI systems of any design would, by default, have incentives to pursue a number of instrumentally useful subgoals, such as acquiring more computing power and amassing many resources, which are harmful even if the overarching goals are not.</li>
</ul>
<p><strong>Safety</strong></p>
<ul>
<li><a href="http://bit.ly/aiphilosophy" target="_blank" rel="noopener">AI Safety: Why It's Hard &amp; State of the Art</a>. I give an overview of AI safety and why the field can be broken down into the four focus areas below (Ethics, Technical Alignment, Cybersecurity, Social Coordination), including a brief overview of the readings in those areas</li>
<li><a href="https://futureoflife.org/landscape/ResearchLandscapeExtended.pdf" target="_blank" rel="noopener">The Landscape of AI Safety and Beneficence Research:</a> Input for Brainstorming at Beneficial AI 2017, Richard Mallah.</li>
<li><a href="http://www.danieldewey.net/fast-takeoff-strategies.html" target="_blank" rel="noopener">Long-term strategies to end existential risk from hard take-off </a>- Daniel Dewey. Covers 4 scenarios: International coordination, sovereign AI, AI-empowered project, Other decisive technological advantage</li>
<li><a href="https://www.youtube.com/watch?v=h0962biiZa4&amp;list=PLpxRpA6hBNrwA8DlvNyIOO9B97wADE1tr" target="_blank" rel="noopener">Beneficial AI Conference</a> -good Youtube channel by FLI</li>
<li><a href="https://www.ted.com/talks/stuart_russell_3_principles_for_creating_safer_ai" target="_blank" rel="noopener">3 Principles for Safe AGI</a> - TED talk by Stuart Russell</li>
<li><a href="https://foresight.org/publications/AGI-Timeframes&amp;PolicyWhitePaper.pdf" target="_blank" rel="noopener">Decentralized Approaches to Reducing Existential Risks </a>- Mark Miller, Christine Peterson, Allison Duettmann on intelligence as problem-solving ability of ecosystems</li>
</ul>
<p><strong>Ethics</strong></p>
<ul>
<li><a href="https://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition" target="_blank" rel="noopener">Coherent Extrapolated Volition</a> - Eliezer Yudkowsky. Good paper to understand why AI safety is hard. <a href="http://lesswrong.com/lw/8iy/objections_to_coherent_extrapolated_volition/" target="_blank" rel="noopener">Objections to Coherent Extrapolated Volition</a> -Lesswrong.</li>
<li><a href="https://futureoflife.org/valuealignmentmap/" target="_blank" rel="noopener">Value Alignment Landscape</a> - Lucas Perry. Interactive, comprehensive overview of the field of value alignment.</li>
</ul>
<p><strong>Technical Alignment</strong></p>
<ul>
<li><a href="https://futureoflife.org/landscape/" target="_blank" rel="noopener">AI Safety Research Landscape</a> - FLI project on mapping AI safety research.</li>
<li><a href="https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/" target="_blank" rel="noopener">AI Alignment: Why it's hard and where to start</a> - Eliezer Yudkowsky. Includes a nice overview of AI alignment</li>
<li><a href="https://ai-alignment.com/" target="_blank" rel="noopener">AI Alignment</a> -Paul Christiano's website on AI alignment. Especially <a href="https://ai-alignment.com/directions-and-desiderata-for-ai-control-b60fca0da8f4" target="_blank" rel="noopener">Directions and desiderata for AI alignment </a>- Paul Christiano</li>
<li><a href="https://intelligence.org/2017/04/12/ensuring/" target="_blank" rel="noopener">Ensuring Smarter-Than-Human Intelligence Has a Positive Outcome</a> talk by Nate Soares</li>
<li><a href="https://futureoflife.org/data/documents/research_priorities.pdf" target="_blank" rel="noopener">Research Priorities for Robust AI</a> - Stuart Russell, Max Tegmark, Daniel Dewey</li>
<li><a href="http://www.fhi.ox.ac.uk/wp-content/uploads/MDL-Intelligence-Distillation-for-safe-superintelligent-problem-solving1.pdf" target="_blank" rel="noopener">Intelligence Distillation</a> - Eric Drexler</li>
</ul>
<p><strong>Social Coordination</strong></p>
<ul>
<li><a href="http://www.allandafoe.com/aireadings" target="_blank" rel="noopener">Reading Guide for the Global Politics of Artificial Intelligence</a> - comprehensive reading list by Allan Dafoe.</li>
<li><a href="https://arxiv.org/pdf/1608.08196.pdf" target="_blank" rel="noopener">Smart Policies for Artificial Intelligence </a>- Miles Brundage, Joanna Bryson</li>
<li><a href="https://poseidon01.ssrn.com/delivery.php?ID=188070086031106064070091006092107110004011091052061061010086120126026068086007097119037011125063116000098101025073082100028104020075007033072006090115086113078025103006065034067014091072064121070083092118108098121103000124011116091107071102114020073116&amp;EXT=pdf" target="_blank" rel="noopener">A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy,</a> Seth Baum</li>
<li><a href="http://www.nickbostrom.com/papers/aipolicy.pdf" target="_blank" rel="noopener">Policy Desiderata in the Development of Machine Superintelligence</a>, Future of Humanity Institute, 2017.	- Bostrom, Nick, Dafoe, Allan, Flynn, Carrick.</li>
<li><a href="https://foresight.org/publications/AGI-Timeframes&amp;PolicyWhitePaper.pdf" target="_blank" rel="noopener">AGI Timelines &amp; Policy White paper</a> Allison Duettmann.</li>
<li><a href="https://www.fhi.ox.ac.uk/wp-content/uploads/Deciphering_Chinas_AI-Dream.pdf" target="_blank" rel="noopener">Deciphering China's AI Dream,</a> Jeffrey Ding</li>
<li><a href="https://medium.com/ai-roadmap-institute/avoiding-the-precipice-db720a805190" target="_blank" rel="noopener">Avoiding the Precipice: Race Avoidance in the Development of AI</a>, Olga Afanasjeva, Jan Feyereisl, Marek Havdra</li>
<li><a href="https://link.springer.com/article/10.1007/s00146-015-0590-y" target="_blank" rel="noopener">Racing to the precipice: A model of artificial intelligence development,</a> Stuart Armstrong, Nick Bostrom, Carl Schulman</li>
<li><a href="https://www.itu.int/en/journal/001/Documents/itu2018-9.pdf" target="_blank" rel="noopener">Beyond MAD?: The Race for Artificial General Intelligence </a>- Roman Yampolskiy</li>
<li><a href="https://hcss.nl/sites/default/files/files/reports/Artificial%20Intelligence%20and%20the%20Future%20of%20Defense.pdf" target="_blank" rel="noopener">AI And The Future of Defense</a> - Stephan De Spiegeleire, Matthijs Maas,, Tim Sweijs</li>
<li><a href="https://www.researchgate.net/publication/324688255_Governing_Boring_Apocalypses_A_New_Typology_of_Existential_Vulnerabilities_and_Exposures_for_Existential_Risk_Research" target="_blank" rel="noopener">Governing Boring Apocalypses: A New Typology of Existential Vulnerabilities and Exposures for Existential Risk Research,</a> Futures, 2018 - Liu, Hin-Yan, Cedervall Lauta, Kristian, Maas, Matth</li>
<li><a href="https://nickbostrom.com/papers/unilateralist.pdf" target="_blank" rel="noopener">Unilateralist's Curse: The Case for a Principle of Conformity </a>by Anders Sandberg, Nick Bostrom, Tom Douglas</li>
<li><a href="https://nickbostrom.com/papers/openness.pdf" target="_blank" rel="noopener">Strategic Implications of Openness in AI Development</a>, Nick Bostrom</li>
<li><a href="https://www.youtube.com/channel/UCFa_X02QhJnP0FNpFAKyRRg" target="_blank" rel="noopener">Livestream</a> of seminar on <a href="https://www.eventbrite.com/e/artificial-general-intelligences-corporations-internet-archive-tickets-44418800829" target="_blank" rel="noopener">Artificial General Intelligences &amp; Corporations @Internet Archive</a>  and <a href="https://drive.google.com/file/d/1LSICJUGV838zYmcYri1tZoDmf2O3y4g7/view?usp=sharing" target="_blank" rel="noopener">Transcript of seminar on AGI &amp; Corporations @ Internet Archive</a></li>
</ul>
<hr>
<h2>Hope</h2>
<ul>
<li><a href="http://slatestarcodex.com/2013/05/06/raikoth-laws-language-and-society/" target="_blank" rel="noopener">Raikoth</a> - An intelligent utopia with perfect language, government, and populace</li>
</ul>
<hr>
<h2>Action</h2>
<p><strong>News</strong></p>
<ul>
<li><a href="http://aisafety.com/" target="_blank" rel="noopener">AIsafety.com</a> - virtual reading list on AI safety, meets every Wednesday</li>
<li><a href="https://www.eff.org/ai/metrics" target="_blank" rel="noopener">EFF AI Metrics </a>- tracking AI progress and development, updated regularly</li>
<li><a href="https://drive.google.com/drive/folders/0B9lGiATYM7jhYWVMTURZM2hXcVk" target="_blank" rel="noopener">Machine Ethics index</a> - Nell Watson. Literature index on machine ethics</li>
<li><a href="https://80000hours.org/articles/ai-policy-guide/" target="_blank" rel="noopener">Guide to working in AI policy</a> - Miles Brundage.</li>
</ul>
<p><strong>AI safety orgs</strong></p>
<ul>
<li><a href="https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/" target="_blank" rel="noopener">pretty comprehensive list</a> by Future of Life Institute on AI safety orgs</li>
<li><a href="https://www.partnershiponai.org/" target="_blank" rel="noopener">Partnership on AI </a>- Established to study and formulate best practices on AI technologies, to advance the public's understanding of AI, and to serve as an open platform for discussion and engagement about AI and its influences on people and society.Established to study and formulate best practices on AI technologies, to advance the public's understanding of AI, and to serve as an open platform for discussion and engagement</li>
<li><a href="https://intelligence.org/" target="_blank" rel="noopener">Machine Intelligence Research Institute</a> - MIRI's artificial intelligence research is focused on developing the mathematical theory of trustworthy reasoning for advanced autonomous AI systems.</li>
<li><a href="http://openai.com/" target="_blank" rel="noopener">OpenAI</a> - OpenAI is a non-profit artificial intelligence research company that aims to promote and develop friendly AI in such a way as to benefit humanity as a whole</li>
<li><a href="https://deepmind.com/" target="_blank" rel="noopener">DeepMind</a> - AI company, bought by Google. Has strong safety focus. Now established Deepmind Ethics &amp; Society Unit</li>
<li><a href="http://humancompatible.ai/" target="_blank" rel="noopener">Center for Human-Compatible AI</a> - CHAI's goal is to develop the conceptual and technical wherewithal to reorient the general thrust of AI research towards provably beneficial systems.</li>
<li><a href="http://lcfi.ac.uk/" target="_blank" rel="noopener">Leverhulme Centre for the Future of Artificial Intelligence</a> - A global community to ensure that AI benefits all of humanity</li>
<li><a href="https://www.fhi.ox.ac.uk/" target="_blank" rel="noopener">Future of Humanity Institute</a> Future of Humanity Institute (FHI) is multidisciplinary research institute working on Existential Risk at the University of Oxford.</li>
<li><a href="https://futureoflife.org/" target="_blank" rel="noopener">Future of Life Institute</a> - a volunteer-run research and outreach organization in the Boston area that works to mitigate existential risks facing humanity, particularly existential risk from advanced artificial intelligence</li>
<li><a href="https://foresight.org/" target="_blank" rel="noopener">Foresight Institute</a> - Foresight Institute is a leading non-profit research organization focused on technologies of fundamental importance for the human future, focusing on molecular machine nanotechnology, cybersecurity, and artificial intelligence.</li>
</ul>
<hr>
<h2>What next?</h2>
<ul>
<li>for a deepdive into AI &amp; Cyberspace, see <a href="https://docs.google.com/document/d/16Uuu395XkuIDLPMA8MNjEuCAvb5o227V3_h5wHBi7-Q/edit#heading=h.3o3ghr1p817" target="_blank" rel="noopener">AI Bonus Material </a></li>
<li>for a reminder of why to care, see <a href="https://docs.google.com/document/d/1R_8EILI3OSVijlavnafaM9nYIyv977SZVJDDDfsXk6M/edit" target="_blank" rel="noopener">Existential Angst &amp; Existential Hope</a></li>
<li>for getting to work, see <a href="https://docs.google.com/document/d/1dVmQw5eT6AeQIT9pTK3Eboebv_QwixRAruiESjCgaPI/edit#" target="_blank" rel="noopener">Effective Altruism</a></li>
<li>for neurotechnology &amp; AI safety, see <a href="https://docs.google.com/document/d/1gyU4NBubq4MQZLbY_gMalW9TsZNWaEloChtDENw97KA/edit#" target="_blank" rel="noopener">Living Longer, Better, Smarter</a></li>
<li>go back to the overview at <a href="https://www.existentialhope.com/" target="_blank" rel="noopener">Existentialhope.com</a> or <a href="https://docs.google.com/forms/d/e/1FAIpQLSfn8JG1uGXDZyKIthvWs_di6kFJJvMa0Py7rky7gguTZhEz4g/viewform" target="_blank" rel="noopener">join the mailing list</a></li>
</ul>
<hr>


            <!-- Tags -->
            


<div class="tags">
    
</div>



            <!-- Comments -->
            <div>
                




            </div>
        </div><!-- end content -->
    </section>
</section><!-- end main -->

<!-- After footer scripts -->

<!-- jQuery -->
<script src="/js/jquery.js"></script>

<!-- Custom Code -->
<script src="/js/main.js"></script>

<!-- Gallery -->
<script src="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->


</body>

</html>